{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good for:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Continuous value predictions\n",
    "2. Relationship between input and output must be linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Outliers can affect the line of best fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have data about a house, such as its square footage, number of bathrooms, whether it has a fireplace or not, the size of its backyard and whether it has a garage or not. We want to predict what the price the house will sell at. \n",
    "\n",
    "The dataset can be found here: https://www.kaggle.com/c/house-prices-advanced-regression-techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression attemps to find the line of best fit in two dimensional data. In higher dimensions it would attempt to find the plane/hyperplane of best fit. \n",
    "\n",
    "The model for linear regression with one feature looks like this:\n",
    "\n",
    "$$h(x) = a_{0} + a_{1}x$$\n",
    "\n",
    "There are two methods of finding the line of best fit. Both methods attempt to find a line to minimize the following error:\n",
    "\n",
    "$$C(x) = \\sum_{i=1}^{n} (y_{i} - h(x_{i}))^2$$\n",
    "\n",
    "Ignoring the squaring, the term in the sum is the difference between what our model predicts should happen at that point ($x_{i}$), and what the actual value in our training data is for that point ($y_{i}$). The sum represents the total error between what our model predicted for each point, and what the actual value was at each point.\n",
    "\n",
    "The reason the term in the sum is squared is because without it, the sign of the error would be taken into account instead of the absolute error. The reason an absolute value wasn't taken is because then the function wouldn't be differentiable and we wouldn't be able to use calculus to find the minimum. \n",
    "\n",
    "If the difference between our model and the actual value is large it'll have a disporportionately larger impact on the cost because of the squaring. As a result, linear regression is more likely to alter the line of best fit to accomodate outliers. \n",
    "\n",
    "This article gives an interpretation of the cost function as variance. Thus what we're really trying to do is reduce the variance of our data, where the average is our model's prediction. https://web.williams.edu/Mathematics/sjmiller/public_html/BrownClasses/54/handouts/MethodLeastSquares.pdf\n",
    "\n",
    "The two methods are:\n",
    "1. Gradient Descent\n",
    "2. Least Squares Method\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent works by attempting to minimize the cost function iteratively. \n",
    "\n",
    "1. It first randomly initializes the coefficients in the model (in this case, $a_{0}$ and $a_{1}$). \n",
    "2. Then it calculates the gradient of the cost function\n",
    "3. It then updates the value of the coefficients to be in the negative direction of gradient\n",
    "\n",
    "Gradient descent then repeats steps 2 and 3 until the successive difference in the cost function is below a particular threshold.\n",
    "\n",
    "Given our above cost function $C(x)$, we can calculate the partial derivatives as:\n",
    "\n",
    "$$\\frac{dC(x)}{da_{0}}\\ = 2*\\sum_{i=1}^{n} (y_{i} - a_{0} - a_{1}x_{i})$$\n",
    "\n",
    "$$\\frac{dC(x)}{da_{1}}\\ = 2*\\sum_{i=1}^{n} -x_{i}(y_{i} - a_{0} - a_{1}x_{i})$$\n",
    "\n",
    "The values of our coefficients get updated as follows:\n",
    "$$a_{0} = a_{0} + \\alpha \\Delta f(a_{0})$$\n",
    "$$a_{1} = a_{1} + \\alpha \\Delta f(a_{1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Least Squares Method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't see why I should attempt to try and do a better job than this article: https://web.williams.edu/Mathematics/sjmiller/public_html/BrownClasses/54/handouts/MethodLeastSquares.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
