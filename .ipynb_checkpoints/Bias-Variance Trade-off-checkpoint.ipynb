{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-Variance Trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias and Variance are terms used in statistics.\n",
    "\n",
    "These terms are used in statistics when attempting to predict a parameter about a population. \n",
    "\n",
    "Take for example the scenario of predicting who will win the next election. In this fictional scenario there are only two parties, liberals and conservatives.\n",
    "\n",
    "If I were to simply open the phonebook and call 50 people and ask them who they're going to vote for, I might get the following data:\n",
    "\n",
    "* liberals: 10\n",
    "* conservatives: 13\n",
    "* no response: 27\n",
    "\n",
    "I can then conclude the conservatives will win. But there are some flaws in this process. \n",
    "\n",
    "This prediction has **high bias** because we only selected those people who are registered in the phonebook. If I were to perform this experiment 50 times, I would still not be able to account for the population that hasn't registered to be in the phonebook.\n",
    "\n",
    "This prediction has **high variance** because our sample size is small. If we were to perform this experiment again, it might turn out that 25 out of the 50 respondents would vote liberal. And so on and so forth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias and Variance have similar meanings in the context of ML as they do in the context of statistical prediction.\n",
    "\n",
    "Imagine you used a different subset of your data to train the same model. \n",
    "\n",
    "If your model wasn't complex enough, say you're using a linear model to approximate a non-linear process, then your model would likely underfit. If you trained the model 100 times, each time on a different subset, the difference between each prediction and the average of all 100 predictions would be fairly low (hence **low variance**), but the difference between the average of all predictions and the actual value would be high (hence **high bias**).\n",
    "\n",
    "If however, your model was too flexible, then your model might overfit. The model would be able to fit all the datapoints, thus encompassing the noise of the process resulting in the inability to generalize and predict targets for new data. If you trained the model 100 times, each time on a different subset, the difference between each prediction and the average of all 100 predictions would be fairly high (**high variance**). This is because, presumably, each subset of data would have varying levels of noise, and the resulting model would fit it perfectly or nearly perfectly, resulting in large differences between each prediction. However, the difference between the average of all predictions and the actual value would be low (hence **low bias**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This wikipedia article has mathematical definitions of bias and variance. The graphic on the right is also very helpful.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
