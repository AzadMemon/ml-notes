{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a sequence of events, what happens next only depends on the current event. Or phrased differently, given a set of states that a system can take on, the next state of the system depends only on the current state. \n",
    "\n",
    "Or phrased mathematically, a stochastic process exhibits the markov property if the probability distribution of the next state depends only on the current state. \n",
    "\n",
    "Take for example a simple creature who eats 3 things: cheese, grapes, bread. What this creature eats today, depends only on what they eat tomorrow. This is an example of a process that has the Markov property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov chains are the simplest of Markov Models. The state of a system is modelled by a random variable that changes through time. \n",
    "\n",
    "### Discrete Markov Chains\n",
    "Let's model the state of our system by a random variable called X. $X_{1}$ represents the state of our system at t = 1, and so on. It's discrete because our time steps are discrete. \n",
    "\n",
    "The next state of the system will depend only on the previous. For example, in our earlier example, if the simple creature ate cheese yesterday, then there's a 0.8 probability that it'll eat bread, and 0.2 probability it'll eat grapes today.\n",
    "\n",
    "$$P(X_{t} = bread | X_{t-1} = cheese) = 0.8$$\n",
    "\n",
    "$$P(X_{t} = grape | X_{t-1} = cheese) = 0.2$$\n",
    "\n",
    "Often Markov chains are time-homogenous. That is, they're independent of t. Probability of our simple creature eating bread after cheese remains the same whether that sequence happens this week or the next.\n",
    "\n",
    "You can also have Markov chains with memory, where it doesn't just represent on the last state, but the last m number of states. \n",
    "\n",
    "We can represent a time-homogenous markov chain using a matrix. Let's call this matrix $P$. We can also further state that if we know the state of the system at $x^{n}$, then the probabilities of the states at $x^{n+3}$ is $x^{n}*P^{3}$.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
