{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good for:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Continuous value predictions\n",
    "2. Relationship between input and output must be linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Outliers can affect the line of best fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have data about a house, such as its square footage, number of bathrooms, whether it has a fireplace or not, the size of its backyard and whether it has a garage or not. We want to predict what the price the house will sell at. \n",
    "\n",
    "The dataset can be found here: https://www.kaggle.com/c/house-prices-advanced-regression-techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression attemps to find the line of best fit in two dimensional data. In higher dimensions it would attempt to find the plane/hyperplane of best fit. \n",
    "\n",
    "The model for linear regression with one feature looks like this:\n",
    "\n",
    "$$h(x) = a_{0} + a_{1}x$$\n",
    "\n",
    "The goal of linear regression is to find a model that best fits the data. It accomplishes this by minimizing the following cost function:\n",
    "\n",
    "$$C(x) = \\sum_{i=0}^{n} (y_{i} - h(x_{i}))^2$$\n",
    "\n",
    "This cost function is actually called many things:\n",
    "* Residual Sum of Squares\n",
    "* Sum of Squared Residuals\n",
    "* Sum of Squared Errors\n",
    "\n",
    "Ignoring the squaring, the term in the sum is the difference between what our model predicts should happen at that point ($x_{i}$), and what the actual value in our training data is for that point ($y_{i}$). The sum represents the total error between what our model predicted for each point, and what the actual value was at each point.\n",
    "\n",
    "The reason the term in the sum is squared is because without it, the sign of the error would be taken into account instead of the absolute error. The reason an absolute value wasn't taken is because then the function wouldn't be differentiable and we wouldn't be able to use calculus to find the minimum. \n",
    "\n",
    "If the difference between our model and the actual value is large it'll have a disporportionately larger impact on the cost because of the squaring. As a result, linear regression is more likely to alter the line of best fit to accomodate outliers. \n",
    "\n",
    "This article gives an interpretation of the cost function as variance. Thus what we're really trying to do is reduce the variance of our data, where the average is our model's prediction. https://web.williams.edu/Mathematics/sjmiller/public_html/BrownClasses/54/handouts/MethodLeastSquares.pdf\n",
    "\n",
    "There are several methods to reduce the above cost function:\n",
    "1. Gradient Descent\n",
    "2. Least Squares Method\n",
    "3. Newton's Method\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent works by attempting to minimize the cost function iteratively. \n",
    "\n",
    "1. It first randomly initializes the coefficients in the model (in this case, $a_{0}$ and $a_{1}$). \n",
    "2. Then it calculates the gradient of the cost function\n",
    "3. It then updates the value of the coefficients to be in the negative direction of gradient\n",
    "\n",
    "The gradient calculates the slope. The slope tells us the rise over the run. So if the slop is -2, it's saying, for every one step in the x-axis you move in the negative direction, the function increases by 2. The sign (negative in this case) tells us what direction the function is moving. Since we're attempting to find a minima, step 3 moves in the negative direction of the gradient.\n",
    "\n",
    "Gradient descent then repeats steps 2 and 3 until the successive difference in the cost function is below a particular threshold.\n",
    "\n",
    "Given our above cost function $C(x)$, we can calculate the partial derivatives as:\n",
    "\n",
    "$$\\frac{dC(x)}{da_{0}}\\ = 2*\\sum_{i=0}^{n} (y_{i} - a_{0} - a_{1}x_{i})$$\n",
    "\n",
    "$$\\frac{dC(x)}{da_{1}}\\ = 2*\\sum_{i=0}^{n} -x_{i}(y_{i} - a_{0} - a_{1}x_{i})$$\n",
    "\n",
    "The values of our coefficients get updated as follows:\n",
    "$$a_{0} = a_{0} + \\alpha \\Delta f(a_{0})$$\n",
    "$$a_{1} = a_{1} + \\alpha \\Delta f(a_{1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Least Squares Method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't see why I should attempt to try and do a better job than this article: https://web.williams.edu/Mathematics/sjmiller/public_html/BrownClasses/54/handouts/MethodLeastSquares.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Newton's Method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newton's method is actually a root-finding algorithm. It uses Taylor's Theorem which approximates a function in the neighborhood of a known point x.\n",
    "\n",
    "To use this for optimization, we calculate the derivative of the approximation, and then find its roots. That is, we're using Newton's method to find the roots of the first-order derivative of our function. This should in theory find us the minima."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
