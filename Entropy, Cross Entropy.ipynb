{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy, Cross Entropy, KL Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-Entropy seems to tie in with several Probability/Statistics/ML concepts.\n",
    "\n",
    "To answer this, I'll need to first touch on several topics:\n",
    "4. Likelihood Functions\n",
    "5. Maximum Likelihood Functions\n",
    "6. Logit functions\n",
    "7. Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In information theory, entropy is the average amount of information gained when you draw one sample from a given probability distribution P. \n",
    "\n",
    "Claude Shannon came up with these theory. He talked about information conveyed in bits. The important part here is to differentiate between:\n",
    "1. bit = binary digit, and \n",
    "2. bit = amount of information conveyed. \n",
    "\n",
    "In the context of his theory, Shannon defined one bit as the amount of information that reduces the recipients uncertainty by a factor of 2. So for example, if there's a 50% chance that it'll rain tomorrow, and a 50% chance it'll be sunshine, when the weather network tells you that its sunny tomorrow, its reduced your uncertainty by a factor of 2. The amount of information the weather network has conveyed to you is 1 bit. Note, the network could have encoded that information using 50 binary digits, but it still only conveyed 1 bits worth of information to you. \n",
    "\n",
    "Some more examples. If there's a 25% chance of a sunny day tomorrow, and 75% chance it'll rain, then when the weather network tells you:\n",
    "1. It's going to be sunny, it has reduced your uncertainty by a factor of 4 i.e. you had 4 equal outcomes, 3 of which were rainy, and one was sunny. The amount of information you gained from this was:\n",
    "$$log_2(uncertainty reduction factor) = log_2(\\frac{1}{probability of event happening}) = log_2(\\frac{1}{0.25}) = log_2(4) = 2 bits$$\n",
    "\n",
    "2. It's going to be rainy, it has reduced your uncertainty by a factor of 1.33. The amount of information you gained from this was:\n",
    "$$log_2(\\frac{1}{0.75}) = log_2(1.33) = 0.41 bits$$\n",
    "\n",
    "Given this information we can calculate entropy: $0.25*2 bits + 0.75*0.41 bits = 0.81 bits$. That is, on average we gain 0.81 bits of information every time the weather network sends us a weather update (given the above probability distribution of weather), or in other words, we gain 0.81 bits of information if we were to randomly sample from the above probability distribution.\n",
    "\n",
    "Hence the **entropy formula**: $H(p) = \\sum_i p_{i}log(p_{i})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Entropy is the average message length. This definition is confusing, but it'll become clear with examples.\n",
    "\n",
    "Imagine we had a distribution of sunny = 0.1, rainy = 0.2, cloudy = 0.3, thunder = 0.4. If we used two bits for each of these events:\n",
    "1. $Entropy = 0.1*log_2(10) + 0.2*log_2(5) + 0.3*log_2(3.33) + 0.4*log_2(2.5) = 1.84 bits$\n",
    "2. $Cross Entropy = 2 bits$\n",
    "\n",
    "By representing sunny with 2 bits, we're implicitly assuming that chances of tomorrow being a sunny day are 1 in 4, or 25%. That is, the number of bits we assign to an event is our predicted probability of that event. Note that ideally, we would use many bits for unlikely events and less bits for likely events. \n",
    "\n",
    "Our predicted distribution is derived from the number of bits we assign to an event, and our actual distribution is the actual probability of that event happening.\n",
    "\n",
    "Our cross entropy is then defined as:\n",
    "$$\\sum_i p_{i}log(q_{i})$$\n",
    "\n",
    "where $q_{i}$ is our predicted probability. This means, the average length of our messages is going to be \"number of bits assigned to that event Multiplied by the probability that event actually happens\".\n",
    "\n",
    "If our cross-entropy and entropy are equal, then we've chosen an optimal encoding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=ErfnhcEV1O8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
